[I 2025-08-11 13:01:00,354] A new study created in RDB with name: gine_optimization
[I 2025-08-11 13:43:50,053] Trial 0 finished with value: 0.08307801811953233 and parameters: {'lr': 0.0001, 'weight_decay': 1e-06, 'dropout': 0.4, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 64, 'epochs': 240}. Best is trial 0 with value: 0.08307801811953233.
[I 2025-08-11 14:12:07,985] Trial 1 finished with value: 0.09077400134261394 and parameters: {'lr': 0.0001, 'weight_decay': 1e-06, 'dropout': 0.4, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 64, 'epochs': 120}. Best is trial 0 with value: 0.08307801811953233.
[I 2025-08-11 14:43:35,959] Trial 2 finished with value: 0.09450014820446775 and parameters: {'lr': 1e-05, 'weight_decay': 1e-06, 'dropout': 0.0, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 32, 'epochs': 300}. Best is trial 0 with value: 0.08307801811953233.
[I 2025-08-11 15:26:54,359] Trial 3 finished with value: 0.0880328934305856 and parameters: {'lr': 1e-05, 'weight_decay': 1e-06, 'dropout': 0.4, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 32, 'epochs': 180}. Best is trial 0 with value: 0.08307801811953233.
[I 2025-08-11 15:58:25,866] Trial 4 finished with value: 0.10270108637316416 and parameters: {'lr': 0.001, 'weight_decay': 1e-05, 'dropout': 0.4, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 64, 'epochs': 180}. Best is trial 0 with value: 0.08307801811953233.
[I 2025-08-11 16:11:23,482] Trial 5 pruned. 
[I 2025-08-11 16:24:07,056] Trial 6 pruned. 
[I 2025-08-11 16:36:50,085] Trial 7 pruned. 
[I 2025-08-11 17:02:44,486] Trial 8 finished with value: 0.09411369251092805 and parameters: {'lr': 0.001, 'weight_decay': 1e-06, 'dropout': 0.0, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 64, 'epochs': 300}. Best is trial 0 with value: 0.08307801811953233.
[I 2025-08-11 17:15:49,386] Trial 9 pruned. 
[I 2025-08-11 17:29:09,133] Trial 10 pruned. 
[I 2025-08-11 17:43:35,341] Trial 11 pruned. 
[I 2025-08-11 17:58:59,669] Trial 12 pruned. 
[I 2025-08-11 18:12:08,726] Trial 13 pruned. 
[I 2025-08-11 18:25:44,204] Trial 14 pruned. 
[I 2025-08-11 18:39:14,581] Trial 15 pruned. 
[I 2025-08-11 18:53:02,780] Trial 16 pruned. 
[I 2025-08-11 19:06:08,025] Trial 17 pruned. 
[I 2025-08-11 19:20:34,369] Trial 18 pruned. 
[I 2025-08-11 19:33:50,335] Trial 19 pruned. 
[I 2025-08-11 19:51:17,011] Trial 20 pruned. 
[I 2025-08-11 20:04:35,031] Trial 21 pruned. 
[I 2025-08-11 20:18:06,358] Trial 22 pruned. 
[I 2025-08-11 20:31:59,950] Trial 23 pruned. 
[I 2025-08-11 20:45:08,866] Trial 24 pruned. 
[I 2025-08-11 20:58:22,718] Trial 25 pruned. 
[I 2025-08-11 22:17:03,438] Trial 26 pruned. 
[I 2025-08-11 22:30:50,583] Trial 27 pruned. 
[I 2025-08-11 22:43:58,237] Trial 28 pruned. 
[I 2025-08-11 22:58:08,231] Trial 29 pruned. 
[I 2025-08-11 23:12:41,648] Trial 30 pruned. 
[I 2025-08-11 23:25:40,437] Trial 31 pruned. 
[I 2025-08-11 23:59:29,339] Trial 32 finished with value: 0.09093503474305556 and parameters: {'lr': 0.001, 'weight_decay': 1e-06, 'dropout': 0.0, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 64, 'epochs': 300}. Best is trial 0 with value: 0.08307801811953233.
[I 2025-08-12 00:12:34,233] Trial 33 pruned. 
[I 2025-08-12 00:25:28,665] Trial 34 pruned. 
[I 2025-08-12 00:38:27,381] Trial 35 pruned. 
[I 2025-08-12 00:51:24,739] Trial 36 pruned. 
[I 2025-08-12 01:04:12,840] Trial 37 pruned. 
[I 2025-08-12 01:17:00,924] Trial 38 pruned. 
[I 2025-08-12 01:29:44,981] Trial 39 pruned. 
[I 2025-08-12 01:42:54,212] Trial 40 pruned. 
[I 2025-08-12 01:55:40,914] Trial 41 pruned. 
[I 2025-08-12 02:17:56,506] Trial 42 finished with value: 0.09310893080948375 and parameters: {'lr': 0.001, 'weight_decay': 1e-06, 'dropout': 0.0, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 64, 'epochs': 300}. Best is trial 0 with value: 0.08307801811953233.
[I 2025-08-12 02:40:27,871] Trial 43 finished with value: 0.09081325708691979 and parameters: {'lr': 0.001, 'weight_decay': 1e-06, 'dropout': 0.0, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 64, 'epochs': 300}. Best is trial 0 with value: 0.08307801811953233.
[I 2025-08-12 02:53:04,866] Trial 44 pruned. 
[I 2025-08-12 03:05:48,928] Trial 45 pruned. 
[I 2025-08-12 03:20:21,710] Trial 46 pruned. 
[I 2025-08-12 03:33:01,286] Trial 47 pruned. 
[I 2025-08-12 03:45:42,272] Trial 48 pruned. 
[I 2025-08-12 03:58:38,345] Trial 49 pruned. 
[I 2025-08-12 04:11:22,036] Trial 50 pruned. 
[I 2025-08-12 04:24:03,824] Trial 51 pruned. 
[I 2025-08-12 04:36:49,729] Trial 52 pruned. 
[I 2025-08-12 04:57:32,503] Trial 53 finished with value: 0.09206421456032024 and parameters: {'lr': 0.001, 'weight_decay': 1e-06, 'dropout': 0.0, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 64, 'epochs': 300}. Best is trial 0 with value: 0.08307801811953233.
[I 2025-08-12 05:10:22,228] Trial 54 pruned. 
[I 2025-08-12 05:23:14,070] Trial 55 pruned. 
[I 2025-08-12 05:35:58,671] Trial 56 pruned. 
[I 2025-08-12 05:48:44,086] Trial 57 pruned. 
[I 2025-08-12 06:01:31,375] Trial 58 pruned. 
[I 2025-08-12 06:14:18,766] Trial 59 pruned. 
Trial done - RMSE log: 0.0852, RMSE original: 0.0030
Trial done - RMSE log: 0.0909, RMSE original: 0.0033
Trial done - RMSE log: 0.0985, RMSE original: 0.0033
Trial done - RMSE log: 0.0952, RMSE original: 0.0032
Trial done - RMSE log: 0.1053, RMSE original: 0.0038
Trial done - RMSE log: 0.0961, RMSE original: 0.0035
Trial done - RMSE log: 0.0926, RMSE original: 0.0033
Trial done - RMSE log: 0.1035, RMSE original: 0.0035
Trial done - RMSE log: 0.0938, RMSE original: 0.0031
Trial done - RMSE log: 0.0937, RMSE original: 0.0032

Best Trial Metrics:
  MAE (log scale): 0.0569
  MSE (log scale): 0.0073
  RMSE (log scale): 0.0852
  R2  (log scale): 0.9821
  MAE (original): 0.0016
  MSE (original): 0.0000
  RMSE (original): 0.0030
  R2  (original): 0.9772

Best hyperparameters:
{'lr': 0.0001, 'weight_decay': 1e-06, 'dropout': 0.4, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 64, 'epochs': 240}

All Completed Trials:
Trial 0: RMSE (log) = 0.0831, RMSE (orig) = 0.0030, Params = {'lr': 0.0001, 'weight_decay': 1e-06, 'dropout': 0.4, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 64, 'epochs': 240}
Trial 1: RMSE (log) = 0.0908, RMSE (orig) = 0.0033, Params = {'lr': 0.0001, 'weight_decay': 1e-06, 'dropout': 0.4, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 64, 'epochs': 120}
Trial 2: RMSE (log) = 0.0945, RMSE (orig) = 0.0033, Params = {'lr': 1e-05, 'weight_decay': 1e-06, 'dropout': 0.0, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 32, 'epochs': 300}
Trial 3: RMSE (log) = 0.0880, RMSE (orig) = 0.0032, Params = {'lr': 1e-05, 'weight_decay': 1e-06, 'dropout': 0.4, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 32, 'epochs': 180}
Trial 4: RMSE (log) = 0.1027, RMSE (orig) = 0.0038, Params = {'lr': 0.001, 'weight_decay': 1e-05, 'dropout': 0.4, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 64, 'epochs': 180}
Trial 8: RMSE (log) = 0.0941, RMSE (orig) = 0.0035, Params = {'lr': 0.001, 'weight_decay': 1e-06, 'dropout': 0.0, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 64, 'epochs': 300}
Trial 32: RMSE (log) = 0.0909, RMSE (orig) = 0.0033, Params = {'lr': 0.001, 'weight_decay': 1e-06, 'dropout': 0.0, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 64, 'epochs': 300}
Trial 42: RMSE (log) = 0.0931, RMSE (orig) = 0.0035, Params = {'lr': 0.001, 'weight_decay': 1e-06, 'dropout': 0.0, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 64, 'epochs': 300}
Trial 43: RMSE (log) = 0.0908, RMSE (orig) = 0.0031, Params = {'lr': 0.001, 'weight_decay': 1e-06, 'dropout': 0.0, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 64, 'epochs': 300}
Trial 53: RMSE (log) = 0.0921, RMSE (orig) = 0.0032, Params = {'lr': 0.001, 'weight_decay': 1e-06, 'dropout': 0.0, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 64, 'epochs': 300}
